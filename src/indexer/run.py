"""
é˜¶æ®µäºŒï¼šåŒºå—é“¾ç´¢å¼•å™¨ï¼ˆIndexerï¼‰
æŒç»­æ‰«æPolymarketäº¤æ˜“äº‹ä»¶å¹¶å­˜å‚¨åˆ°æ•°æ®åº“

åŠŸèƒ½ï¼š
1. è·å– OrderFilled äº‹ä»¶æ—¥å¿—
2. è§£æäº¤æ˜“æ•°æ®
3. å­˜å‚¨åˆ°æ•°æ®åº“
4. æ”¯æŒå¢é‡åŒæ­¥å’ŒæŒç»­ç›‘å¬
"""
import logging
import sys
import json
import time
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import argparse
from dotenv import load_dotenv
import os
from web3 import Web3
from decimal import Decimal

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from trade_decoder import TradeDecoder, Trade
from market_decoder import MarketDecoder
from db.schema import init_db, get_connection
from indexer.store import DataStore
from indexer.gamma import GammaAPIClient

load_dotenv()
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PolymarketIndexer:
    """Polymarketäº¤æ˜“ç´¢å¼•å™¨"""
    
    # äº¤æ˜“æ‰€åœ°å€
    CTF_EXCHANGE = "0x4bFb41d5B3570DeFd03C39a9A4D8dE6Bd8B8982E"
    NEG_RISK_EXCHANGE = "0xC5d563A36AE78145C45a50134d48A1215220f80a"
    
    # OrderFilled äº‹ä»¶ç­¾å
    # event OrderFilled(bytes32 indexed orderHash, address indexed maker, address indexed taker, 
    #                   uint256 makerAssetId, uint256 takerAssetId, uint256 makerAmountFilled, 
    #                   uint256 takerAmountFilled, uint256 fee)
    ORDER_FILLED_TOPIC = "0xd0a08e8c493f9c94f29311604c9de1b4e8c8d4c06bd0c789af57f2d65bfec0f6"
    
    # æ‰¹å¤„ç†å¤§å°ï¼ˆåŒºå—æ•°ï¼‰- Polygon RPC é™åˆ¶æ¯æ¬¡è¿”å› 10000 æ¡æ—¥å¿—
    # Polymarket äº¤æ˜“é‡å¤§ï¼Œéœ€è¦è¾ƒå°çš„æ‰¹æ¬¡
    BATCH_SIZE = 50
    
    # Polygon å‡ºå—é—´éš”ï¼ˆç§’ï¼‰
    BLOCK_TIME = 2
    
    def __init__(self, rpc_url: str, db_path: str = "data/polymarket.db"):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨
        
        Args:
            rpc_url: Polygon RPC URL
            db_path: SQLite æ•°æ®åº“è·¯å¾„
        """
        logger.info("åˆå§‹åŒ–ç´¢å¼•å™¨...")
        
        self.rpc_url = rpc_url
        self.db_path = db_path
        self.web3 = Web3(Web3.HTTPProvider(rpc_url))
        
        # éªŒè¯RPCè¿æ¥
        if not self.web3.is_connected():
            raise ConnectionError("æ— æ³•è¿æ¥åˆ° Polygon RPC")
        
        chain_id = self.web3.eth.chain_id
        logger.info(f"âœ“ RPC è¿æ¥æˆåŠŸ: é“¾ID {chain_id}")
        
        if chain_id != 137:
            logger.warning(f"âš  é Polygon ä¸»ç½‘ (é“¾ID: {chain_id})")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        init_db(db_path)
        logger.info(f"âœ“ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {db_path}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.store = DataStore(db_path)
        self.trade_decoder = TradeDecoder(rpc_url)
        self.market_decoder = MarketDecoder()
        self.gamma_client = GammaAPIClient()
        
        logger.info("âœ“ ç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_current_block(self) -> int:
        """è·å–å½“å‰åŒºå—é«˜åº¦"""
        return self.web3.eth.block_number
    
    def get_block_timestamp(self, block_number: int) -> Optional[datetime]:
        """è·å–åŒºå—æ—¶é—´æˆ³"""
        try:
            block = self.web3.eth.get_block(block_number)
            return datetime.fromtimestamp(block['timestamp'])
        except Exception as e:
            logger.warning(f"è·å–åŒºå—æ—¶é—´æˆ³å¤±è´¥: {e}")
            return None
    
    def fetch_order_filled_logs(self, from_block: int, to_block: int) -> List[Dict]:
        """
        è·å– OrderFilled äº‹ä»¶æ—¥å¿—
        
        Args:
            from_block: å¼€å§‹åŒºå—
            to_block: ç»“æŸåŒºå—
            
        Returns:
            æ—¥å¿—åˆ—è¡¨
        """
        try:
            logs = self.web3.eth.get_logs({
                'fromBlock': from_block,
                'toBlock': to_block,
                'address': [
                    Web3.to_checksum_address(self.CTF_EXCHANGE),
                    Web3.to_checksum_address(self.NEG_RISK_EXCHANGE)
                ],
                'topics': [self.ORDER_FILLED_TOPIC]
            })
            
            logger.debug(f"è·å–åˆ° {len(logs)} ä¸ª OrderFilled äº‹ä»¶")
            return list(logs)
        
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—å¤±è´¥ ({from_block}-{to_block}): {e}")
            return []
    
    def parse_log_to_trade(self, log: Dict) -> Optional[Tuple[Trade, int]]:
        """
        ç›´æ¥è§£æå•ä¸ªæ—¥å¿—ä¸º Trade å¯¹è±¡
        
        Args:
            log: æ—¥å¿—å¯¹è±¡
            
        Returns:
            (Trade å¯¹è±¡, block_number) æˆ– None
        """
        try:
            tx_hash = log.get('transactionHash')
            if hasattr(tx_hash, 'hex'):
                tx_hash = tx_hash.hex()
            
            log_index = log.get('logIndex', 0)
            block_number = log.get('blockNumber', 0)
            
            # ä½¿ç”¨ trade_decoder çš„è§£ææ–¹æ³•
            trade = self.trade_decoder._parse_order_filled_log(tx_hash, log_index, log)
            
            if trade:
                return (trade, block_number)
            
            return None
            
        except Exception as e:
            logger.warning(f"è§£ææ—¥å¿—å¤±è´¥: {e}")
            return None
    
    def process_logs_batch(self, logs: List[Dict]) -> List[Tuple[Trade, int]]:
        """
        æ‰¹é‡å¤„ç†æ—¥å¿—
        
        Args:
            logs: æ—¥å¿—åˆ—è¡¨
            
        Returns:
            [(Trade, block_number), ...] åˆ—è¡¨
        """
        trades = []
        
        for log in logs:
            result = self.parse_log_to_trade(log)
            if result:
                trades.append(result)
        
        return trades
    
    def enrich_trades_with_market(self, trades: List[Tuple[Trade, int]]) -> List[Dict]:
        """
        ä½¿ç”¨å¸‚åœºä¿¡æ¯ä¸°å¯Œäº¤æ˜“æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸ºå­—å…¸
        
        Args:
            trades: [(Trade, block_number), ...] åˆ—è¡¨
            
        Returns:
            ä¸°å¯Œåçš„äº¤æ˜“å­—å…¸åˆ—è¡¨
        """
        # è·å–æ‰€æœ‰å¸‚åœºçš„ token ID æ˜ å°„
        token_to_market = self.store.get_token_to_market_mapping()
        
        result = []
        for trade, block_number in trades:
            trade_dict = {
                'tx_hash': trade.tx_hash,
                'log_index': trade.log_index,
                'exchange': trade.exchange,
                'order_hash': trade.order_hash,
                'maker': trade.maker,
                'taker': trade.taker,
                'maker_asset_id': trade.maker_asset_id,
                'taker_asset_id': trade.taker_asset_id,
                'maker_amount': trade.maker_amount,
                'taker_amount': trade.taker_amount,
                'fee': trade.fee,
                'price': trade.price,
                'token_id': trade.token_id,
                'side': trade.side,
                'block_number': block_number,
                'market_slug': None,
                'condition_id': None,
                'outcome': None
            }
            
            # å°è¯•å…³è”å¸‚åœºä¿¡æ¯
            token_id = trade.token_id
            if token_id in token_to_market:
                market = token_to_market[token_id]
                trade_dict['market_slug'] = market.get('slug')
                trade_dict['condition_id'] = market.get('condition_id')
                trade_dict['outcome'] = market.get('outcome')
            
            result.append(trade_dict)
        
        return result
    
    def store_trades(self, trade_dicts: List[Dict]) -> int:
        """
        å­˜å‚¨äº¤æ˜“åˆ°æ•°æ®åº“
        
        Args:
            trade_dicts: äº¤æ˜“å­—å…¸åˆ—è¡¨
            
        Returns:
            æˆåŠŸå­˜å‚¨çš„äº¤æ˜“æ•°
        """
        if not trade_dicts:
            return 0
        
        return self.store.insert_trades(trade_dicts)
    
    def sync_markets_from_gamma(self, limit: int = 100) -> int:
        """
        ä» Gamma API åŒæ­¥çƒ­é—¨å¸‚åœº
        
        Args:
            limit: è·å–å¸‚åœºæ•°é‡
            
        Returns:
            åŒæ­¥çš„å¸‚åœºæ•°é‡
        """
        try:
            logger.info(f"ä» Gamma API åŒæ­¥å¸‚åœº (limit={limit})...")
            
            markets = self.gamma_client.fetch_active_markets(limit=limit)
            
            if not markets:
                logger.warning("æœªè·å–åˆ°å¸‚åœºæ•°æ®")
                return 0
            
            synced = 0
            for market_data in markets:
                try:
                    # ä½¿ç”¨ market_decoder è®¡ç®— token IDs
                    condition_id = market_data.get('conditionId')
                    if not condition_id:
                        continue
                    
                    # å°è¯•ä» clobTokenIds è·å–
                    clob_tokens = market_data.get('clobTokenIds')
                    if clob_tokens:
                        try:
                            token_ids = json.loads(clob_tokens) if isinstance(clob_tokens, str) else clob_tokens
                            yes_token = token_ids[0] if len(token_ids) > 0 else None
                            no_token = token_ids[1] if len(token_ids) > 1 else None
                        except:
                            yes_token = None
                            no_token = None
                    else:
                        # ä½¿ç”¨ market_decoder è®¡ç®—
                        try:
                            yes_token, no_token = self.market_decoder.derive_token_ids(condition_id)
                        except:
                            yes_token = None
                            no_token = None
                    
                    market_record = {
                        'slug': market_data.get('slug'),
                        'question': market_data.get('question'),
                        'condition_id': condition_id,
                        'yes_token_id': yes_token,
                        'no_token_id': no_token,
                        'oracle': market_data.get('marketMakerAddress'),
                        'collateral_token': '0x2791Bca1f2de4661ED88A30C99A7a9449Aa84174',  # USDC.e
                        'category': market_data.get('category'),
                        'end_date': market_data.get('endDate'),
                        'volume': market_data.get('volumeNum', 0),
                        'liquidity': market_data.get('liquidityNum', 0),
                        'active': market_data.get('active', True),
                        'closed': market_data.get('closed', False),
                        'resolved': False
                    }
                    
                    self.store.upsert_market(market_record)
                    synced += 1
                    
                except Exception as e:
                    logger.warning(f"åŒæ­¥å¸‚åœºå¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ“ åŒæ­¥å®Œæˆ: {synced} ä¸ªå¸‚åœº")
            return synced
            
        except Exception as e:
            logger.error(f"åŒæ­¥å¸‚åœºå¤±è´¥: {e}")
            return 0
    
    def run_batch(self, from_block: int, to_block: int) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡
        
        Args:
            from_block: èµ·å§‹åŒºå—
            to_block: ç»“æŸåŒºå—
            
        Returns:
            æ‰¹æ¬¡å¤„ç†ç»“æœ
        """
        result = {
            'from_block': from_block,
            'to_block': to_block,
            'logs_found': 0,
            'trades_parsed': 0,
            'trades_stored': 0
        }
        
        # 1. è·å–æ—¥å¿—
        logs = self.fetch_order_filled_logs(from_block, to_block)
        result['logs_found'] = len(logs)
        
        if not logs:
            return result
        
        # 2. è§£æäº¤æ˜“
        trade_tuples = self.process_logs_batch(logs)
        result['trades_parsed'] = len(trade_tuples)
        
        if not trade_tuples:
            return result
        
        # 3. ä¸°å¯Œå¸‚åœºä¿¡æ¯å¹¶è½¬æ¢ä¸ºå­—å…¸
        trade_dicts = self.enrich_trades_with_market(trade_tuples)
        
        # 4. å­˜å‚¨
        stored = self.store_trades(trade_dicts)
        result['trades_stored'] = stored
        
        return result
    
    def run_indexer(self, 
                    from_block: Optional[int] = None,
                    to_block: Optional[int] = None,
                    continuous: bool = False,
                    sync_markets: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œç´¢å¼•å™¨
        
        Args:
            from_block: èµ·å§‹åŒºå—ï¼ˆé»˜è®¤ä»åŒæ­¥çŠ¶æ€è¯»å–ï¼‰
            to_block: ç»“æŸåŒºå—ï¼ˆé»˜è®¤ä¸ºæœ€æ–°åŒºå—ï¼‰
            continuous: æ˜¯å¦æŒç»­è¿è¡Œ
            sync_markets: æ˜¯å¦å…ˆåŒæ­¥å¸‚åœºæ•°æ®
            
        Returns:
            è¿è¡Œç»“æœç»Ÿè®¡
        """
        logger.info("=" * 60)
        logger.info("  ğŸš€ å¯åŠ¨ Polymarket ç´¢å¼•å™¨")
        logger.info("=" * 60)
        
        # åŒæ­¥å¸‚åœºæ•°æ®
        if sync_markets:
            self.sync_markets_from_gamma(limit=100)
        
        # ç¡®å®šåŒºå—èŒƒå›´
        current_block = self.get_current_block()
        
        if from_block is None:
            sync_state = self.store.get_sync_state()
            from_block = sync_state.get('last_block', current_block - 1000)
        
        if to_block is None:
            to_block = current_block
        
        logger.info(f"ğŸ“Š å¤„ç†åŒºå—èŒƒå›´: {from_block:,} - {to_block:,} ({to_block - from_block:,} ä¸ªåŒºå—)")
        
        # ç»Ÿè®¡
        stats = {
            'status': 'running',
            'start_block': from_block,
            'end_block': to_block,
            'total_logs': 0,
            'total_trades_parsed': 0,
            'total_trades_stored': 0,
            'batches_processed': 0,
            'start_time': datetime.now().isoformat()
        }
        
        # åˆ†æ‰¹å¤„ç†
        batch_start = from_block
        
        while batch_start <= to_block:
            batch_end = min(batch_start + self.BATCH_SIZE - 1, to_block)
            
            try:
                logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡: {batch_start:,} - {batch_end:,}")
                
                result = self.run_batch(batch_start, batch_end)
                
                stats['total_logs'] += result['logs_found']
                stats['total_trades_parsed'] += result['trades_parsed']
                stats['total_trades_stored'] += result['trades_stored']
                stats['batches_processed'] += 1
                
                if result['logs_found'] > 0:
                    logger.info(f"   âœ“ æ—¥å¿—: {result['logs_found']}, "
                               f"è§£æ: {result['trades_parsed']}, "
                               f"å­˜å‚¨: {result['trades_stored']}")
                
                # æ›´æ–°åŒæ­¥çŠ¶æ€
                self.store.update_sync_state(batch_end, stats['total_trades_stored'])
                
                batch_start = batch_end + 1
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                if not continuous:
                    stats['status'] = 'error'
                    stats['error'] = str(e)
                    return stats
                else:
                    # æŒç»­æ¨¡å¼ä¸‹è·³è¿‡å¤±è´¥æ‰¹æ¬¡
                    batch_start = batch_end + 1
                    continue
        
        stats['status'] = 'completed'
        stats['end_time'] = datetime.now().isoformat()
        
        logger.info("=" * 60)
        logger.info(f"âœ… ç´¢å¼•å®Œæˆ!")
        logger.info(f"   æ€»æ—¥å¿—: {stats['total_logs']:,}")
        logger.info(f"   è§£æäº¤æ˜“: {stats['total_trades_parsed']:,}")
        logger.info(f"   å­˜å‚¨äº¤æ˜“: {stats['total_trades_stored']:,}")
        logger.info("=" * 60)
        
        # æŒç»­ç›‘å¬æ¨¡å¼
        if continuous:
            logger.info("ğŸ”„ è¿›å…¥æŒç»­ç›‘å¬æ¨¡å¼...")
            last_processed_block = to_block
            
            while True:
                try:
                    time.sleep(self.BLOCK_TIME)
                    
                    latest_block = self.get_current_block()
                    
                    if latest_block > last_processed_block:
                        new_from = last_processed_block + 1
                        logger.info(f"ğŸ†• å‘ç°æ–°åŒºå—: {new_from} - {latest_block}")
                        
                        result = self.run_batch(new_from, latest_block)
                        
                        stats['total_logs'] += result['logs_found']
                        stats['total_trades_parsed'] += result['trades_parsed']
                        stats['total_trades_stored'] += result['trades_stored']
                        
                        if result['trades_stored'] > 0:
                            logger.info(f"   âœ“ æ–°å­˜å‚¨ {result['trades_stored']} ç¬”äº¤æ˜“")
                        
                        self.store.update_sync_state(latest_block, stats['total_trades_stored'])
                        last_processed_block = latest_block
                
                except KeyboardInterrupt:
                    logger.info("â¹ ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢ç´¢å¼•å™¨")
                    break
                except Exception as e:
                    logger.error(f"æŒç»­ç›‘å¬å‡ºé”™: {e}")
                    continue
            
            stats['status'] = 'stopped'
            stats['end_time'] = datetime.now().isoformat()
        
        return stats


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="Polymarket åŒºå—é“¾ç´¢å¼•å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ‰«ææœ€è¿‘ 1000 ä¸ªåŒºå—
  python run.py
  
  # ä»æŒ‡å®šåŒºå—å¼€å§‹
  python run.py --from-block 50000000
  
  # æŒç»­ç›‘å¬æ–°åŒºå—
  python run.py --continuous
  
  # åªåŒæ­¥å¸‚åœºæ•°æ®
  python run.py --sync-markets-only
"""
    )
    parser.add_argument(
        "--rpc-url",
        type=str,
        default=os.getenv("RPC_URL", "https://polygon-rpc.com"),
        help="Polygon RPC URL"
    )
    parser.add_argument(
        "--db",
        type=str,
        default=os.getenv("DB_PATH", "data/polymarket.db"),
        help="SQLite æ•°æ®åº“è·¯å¾„"
    )
    parser.add_argument(
        "--from-block",
        type=int,
        default=None,
        help="èµ·å§‹åŒºå—ï¼ˆé»˜è®¤ä»åŒæ­¥çŠ¶æ€è¯»å–ï¼‰"
    )
    parser.add_argument(
        "--to-block",
        type=int,
        default=None,
        help="ç»“æŸåŒºå—ï¼ˆé»˜è®¤ä¸ºæœ€æ–°åŒºå—ï¼‰"
    )
    parser.add_argument(
        "--continuous",
        action="store_true",
        help="æŒç»­ç›‘å¬æ–°åŒºå—"
    )
    parser.add_argument(
        "--no-sync-markets",
        action="store_true",
        help="è·³è¿‡å¸‚åœºæ•°æ®åŒæ­¥"
    )
    parser.add_argument(
        "--sync-markets-only",
        action="store_true",
        help="åªåŒæ­¥å¸‚åœºæ•°æ®ï¼Œä¸æ‰«æåŒºå—"
    )
    
    args = parser.parse_args()
    
    try:
        indexer = PolymarketIndexer(
            rpc_url=args.rpc_url,
            db_path=args.db
        )
        
        if args.sync_markets_only:
            # åªåŒæ­¥å¸‚åœº
            synced = indexer.sync_markets_from_gamma(limit=100)
            logger.info(f"åŒæ­¥å®Œæˆ: {synced} ä¸ªå¸‚åœº")
        else:
            # è¿è¡Œç´¢å¼•å™¨
            result = indexer.run_indexer(
                from_block=args.from_block,
                to_block=args.to_block,
                continuous=args.continuous,
                sync_markets=not args.no_sync_markets
            )
            
            logger.info(f"æœ€ç»ˆç»“æœ: {json.dumps(result, indent=2, default=str)}")
    
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç´¢å¼•å™¨è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
